{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50dc21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9ab9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phucle/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torchvision/utils.py:233: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../datasets/video_data/images/frame_0-00-27.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-19.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-42.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-38.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-06.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-49.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-12.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-33.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-56.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-09.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-37.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-52.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-16.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-28.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-59.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-02.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-23.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-46.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-39.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-07.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-43.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-26.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-18.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-57.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-32.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-13.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-48.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-17.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-29.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-53.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-08.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-36.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-47.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-22.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-03.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-58.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-54.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-31.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-10.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-04.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-40.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-25.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-01-00.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-44.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-21.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-00.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-14.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-50.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-35.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-11.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-30.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-55.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-24.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-41.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-05.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-01.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-20.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-45.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-34.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-51.000_interval.jpg\n",
      "Processed ../datasets/video_data/images/frame_0-00-15.000_interval.jpg\n",
      "Processed 61 images.\n",
      "\n",
      "Total inference time: 76.02 seconds\n",
      "Average inference time per image: 1.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/vision/main/models.html\n",
    "\n",
    "weights = FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fcos_resnet50_fpn(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "image_paths = glob.glob(\"../datasets/video_data/images/*.jpg\")\n",
    "preprocess = weights.transforms()\n",
    "threshold = 0.45\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for image in image_paths:\n",
    "    img = read_image(image)\n",
    "    processed = preprocess(img)\n",
    "    prediction = model([processed])[0]\n",
    "\n",
    "    keep = prediction['scores'] > threshold\n",
    "    boxes = prediction['boxes'][keep]\n",
    "    labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"][keep]]\n",
    "\n",
    "    drawn = draw_bounding_boxes(img.cpu(), boxes=boxes, labels=labels, colors=\"red\", width=2, font_size=30)\n",
    "    output_image = to_pil_image(drawn)\n",
    "\n",
    "    output_image.save(\"../output/fcos_normal/\" + os.path.basename(image))\n",
    "    print(f\"Processed {image}\") \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "average_time = total_time / len(image_paths)\n",
    "\n",
    "print(f\"Processed {len(image_paths)} images.\")\n",
    "print(f\"\\nTotal inference time: {total_time:.2f} seconds\")\n",
    "print(f\"Average inference time per image: {average_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
