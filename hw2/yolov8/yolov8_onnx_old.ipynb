{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b775fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import yaml_load\n",
    "from ultralytics.utils.checks import check_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c585d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.118 ðŸš€ Python-3.12.3 torch-2.7.0 CPU (Apple M2)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.4s, saved as 'yolov8n.onnx' (12.1 MB)\n",
      "\n",
      "Export complete (6.6s)\n",
      "Results saved to \u001b[1m/Users/phucle/Desktop/cmpe258/hw/hw2/yolov8\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8n.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/@zain.18j2000/how-to-use-custom-or-official-yolov8-object-detection-model-in-onnx-format-ca8f055643df\n",
    "# https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/main.py\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "output_folder = \"../output/yolov8_onnx\"\n",
    "model.export(format=\"onnx\", opset = 12, dynamic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = 'yolov8n.onnx'\n",
    "folder_path = \"../datasets/video_data/images\"\n",
    "output_folder = \"../output/yolov8_onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a4ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, input_size=(640, 640)):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image {image_path}\")\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = img_rgb.shape[:2]\n",
    "    img_resized = cv2.resize(img_rgb, input_size)\n",
    "    img_transposed = img_resized.transpose(2, 0, 1)\n",
    "    img_input = img_transposed.reshape(1, 3, *input_size).astype(np.float32) / 255.0\n",
    "    return image, img_input, img_w, img_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accc6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(session, input_blob):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    outputs = session.run(None, {input_name: input_blob})\n",
    "    return outputs[0][0].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b729c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(results, thresh=0.5):\n",
    "    if results.shape[1] == 5:\n",
    "        detections = results[results[:, 4] > thresh]\n",
    "    else:\n",
    "        temp = []\n",
    "        for det in results:\n",
    "            class_id = det[4:].argmax()\n",
    "            confidence = det[4:].max()\n",
    "            temp.append(np.append(det[:4], [class_id, confidence]))\n",
    "        temp = np.array(temp)\n",
    "        detections = temp[temp[:, -1] > thresh]\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_boxes(results, img_w, img_h, input_size=(640, 640)):\n",
    "    cx, cy, w, h, class_id, conf = results[:, 0], results[:, 1], results[:, 2], results[:, 3], results[:, 4], results[:, 5]\n",
    "    cx = cx / input_size[0] * img_w\n",
    "    cy = cy / input_size[1] * img_h\n",
    "    w = w / input_size[0] * img_w\n",
    "    h = h / input_size[1] * img_h\n",
    "\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "    x2 = cx + w / 2\n",
    "    y2 = cy + h / 2\n",
    "\n",
    "    boxes = np.stack([x1, y1, x2, y2], axis=1)\n",
    "    classes = class_id.astype(int)\n",
    "    return boxes, classes, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5379715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, classes, scores, class_names):\n",
    "    for box, cls_id, score in zip(boxes, classes, scores):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = f\"{class_names[cls_id]} {score:.2f}\"\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f157c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(boxes, confidences, score_threshold=0.45, iou_threshold=0.7):\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes.tolist(),\n",
    "        scores=confidences.tolist(),\n",
    "        score_threshold=score_threshold,\n",
    "        nms_threshold=iou_threshold\n",
    "    )\n",
    "    indices = indices.flatten() if len(indices) > 0 else []\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b48749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, output_folder, onnx_path):\n",
    "    classes = yaml_load(check_yaml(\"coco8.yaml\"))[\"names\"]\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.jpg')]\n",
    "\n",
    "    total_inference_time = 0\n",
    "    total_images = len(image_files)\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        original_image, img_input, img_w, img_h = preprocess_image(image_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "        outputs = run_inference(session, img_input)\n",
    "        end_time = time.time()\n",
    "\n",
    "        inference_time = end_time - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "        filtered_results = filter_detections(outputs)\n",
    "        boxes, class_ids, confidences = rescale_boxes(filtered_results, img_w, img_h)\n",
    "        boxes_for_nms = [[int(x1), int(y1), int(x2-x1), int(y2-y1)] for x1, y1, x2, y2 in boxes]\n",
    "        keep_indices = apply_nms(np.array(boxes_for_nms), confidences)\n",
    "        final_boxes = boxes[keep_indices]\n",
    "        final_class_ids = class_ids[keep_indices]\n",
    "        final_confidences = confidences[keep_indices]\n",
    "        output_image = draw_boxes(original_image, final_boxes, final_class_ids, final_confidences, classes)\n",
    "        output_path = os.path.join(output_folder, image_file)\n",
    "        cv2.imwrite(output_path, output_image)\n",
    "\n",
    "    print(f\"Processed {total_images} images.\")\n",
    "    print(f\"\\nTotal inference time: images: {total_inference_time:.2f} seconds\")\n",
    "    print(f\"Average inference time per image: {total_inference_time/total_images:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6793377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 61 images.\n",
      "\n",
      "Total inference time: images: 2.23 seconds\n",
      "Average inference time per image: 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "process_folder(folder_path, output_folder, onnx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
